{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dy6Ma1jJIk-"
   },
   "source": [
    "# Overview\n",
    "\n",
    "This notebook demonstrates the multimodal [Vertex AI Embedding API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-image-embeddings)\n",
    "\n",
    "Input:\n",
    "- A BigQuery table with a text column and GCS URIs\n",
    "\n",
    "Output:\n",
    "- Multimodal embeddings stored back in the in the BiqQuery table\n",
    "\n",
    "\n",
    "It is important to note that text and image data is not fused into a single embedding. The service provides text-only and image-only embeddings, but they share an embedding space. In other words the text ‘cat’ and a picture of a cat should return similar embeddings. This is useful because it allows us at inference time to operate with text-only or image-only inputs instead of requiring both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFZFdk1tJIk_"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FNPoyASex78"
   },
   "source": [
    "### Install Dependencies (If Needed)\n",
    "\n",
    "The list `packages` contains tuples of package import names and install names. If the import name is not found then the install name is used to install quitely for the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XP0gmJTkWQs9"
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform'),\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p23YtY2ze8S_"
   },
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nF9O2bzVCzmg"
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cVyw7HafDZp"
   },
   "source": [
    "### Authenticate\n",
    "\n",
    "If you are using Colab, you will need to authenticate yourself first. The next cell will check if you are currently using Colab, and will start the authentication process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liPfoHL8fDAZ"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import auth as google_auth\n",
    "    google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzT0Vn6pfbJb"
   },
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XSgAq3jfJIk_"
   },
   "outputs": [],
   "source": [
    "PROJECT = 'solutions-2023-mar-107'\n",
    "LOCATION = 'us-central1'\n",
    "BQ_TABLE = 'solutions-2023-mar-107.mercari.13K_synthetic_attributes_embeddings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kELokCl9JIk_"
   },
   "source": [
    "# Online Embedding API\n",
    "\n",
    "Client code for [multimodal embedding API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#api-usage), we reproduce it below with minor modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_JW2BCAjVf7"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import struct_pb2\n",
    "from functools import cache\n",
    "import time\n",
    "import typing\n",
    "\n",
    "\n",
    "class EmbeddingResponse(typing.NamedTuple):\n",
    "  text_embedding: typing.Sequence[float]\n",
    "  image_embedding: typing.Sequence[float]\n",
    "\n",
    "\n",
    "class EmbeddingPredictionClient:\n",
    "  \"\"\"Wrapper around Prediction Service Client.\"\"\"\n",
    "  def __init__(self, project : str,\n",
    "    location : str = \"us-central1\",\n",
    "    api_regional_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\n",
    "    client_options = {\"api_endpoint\": api_regional_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    self.client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    self.location = location\n",
    "    self.project = project\n",
    "\n",
    "  def get_embedding(self, text : str = None, image_path : str = None):\n",
    "    \"\"\"image_path can be a local path or a GCS URI.\"\"\"\n",
    "    if not text and not image_path:\n",
    "      raise ValueError('At least one of text or image_bytes must be specified.')\n",
    "\n",
    "    instance = struct_pb2.Struct()\n",
    "    if text:\n",
    "      if len(text) > 1024:\n",
    "        warnings.warn('Text must be less than 1024 characters.')\n",
    "        text = text[:1023],
    "        print(f'truncated the text to {len(text)} characters')\n",
    "      instance.fields['text'].string_value = text\n",
    "\n",
    "    if image_path:\n",
    "      image_struct = instance.fields['image'].struct_value\n",
    "      if image_path.lower().startswith('gs://'):\n",
    "        image_struct.fields['gcsUri'].string_value = image_path\n",
    "      else:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "          image_bytes = f.read()\n",
    "        encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "        image_struct.fields['bytesBase64Encoded'].string_value = encoded_content\n",
    "\n",
    "    instances = [instance]\n",
    "    endpoint = (f\"projects/{self.project}/locations/{self.location}\"\n",
    "      \"/publishers/google/models/multimodalembedding@001\")\n",
    "    try:\n",
    "      response = self.client.predict(endpoint=endpoint, instances=instances)\n",
    "\n",
    "      text_embedding = None\n",
    "      if text:\n",
    "        text_emb_value = response.predictions[0]['textEmbedding']\n",
    "        text_embedding = [v for v in text_emb_value]\n",
    "\n",
    "      image_embedding = None\n",
    "      if image_path:\n",
    "        image_emb_value = response.predictions[0]['imageEmbedding']\n",
    "        image_embedding = [v for v in image_emb_value]\n",
    "\n",
    "      return EmbeddingResponse(\n",
    "        text_embedding=text_embedding,\n",
    "        image_embedding=image_embedding)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      return None\n",
    "    \n",
    "@cache\n",
    "def get_client(project):\n",
    "  return EmbeddingPredictionClient(project)\n",
    "\n",
    "\n",
    "def embed(project,text,image_path=None):\n",
    "  client = get_client(project)\n",
    "  start = time.time()\n",
    "  response = client.get_embedding(text=text, image_path=image_path)\n",
    "  end = time.time()\n",
    "  print('Embedding Time: ', end - start)\n",
    "  return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79IuogmoJIk_"
   },
   "source": [
    "### Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "P83VI2tmJIk_"
   },
   "outputs": [],
   "source": [
    "res = embed(PROJECT,\n",
    "            'IZOD Women\\'s Light Gray Thigh Length Pull On Golf Shorts',\n",
    "            'gs://genai-product-catalog/toy_images/shorts.jpg')\n",
    "\n",
    "print(res.text_embedding[:5])\n",
    "print(res.image_embedding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDvkPi3aJIlA"
   },
   "source": [
    "# Add Embeddings to BQ\n",
    "\n",
    "This is a naive approach which loads BQ rows sequentially and embeds using the online embedding API. We use pagination to avoid OOM errors for large datasets.\n",
    "\n",
    "We batch BQ row updates for efficiency. The embedding API is rate limited at 120 req/min so this is as fast as we can go.\n",
    "\n",
    "In the future, pending product updates, there may be more efficient options:\n",
    "1. Embed directly from BQ. This is supported for textembedding-gecko today (https://cloud.google.com/blog/products/data-analytics/introducing-bigquery-text-embeddings) and if support roles out for multimodal this would be the ideal choice\n",
    "2. Once a batch embedding API is available use that instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "y0x5J6PLJIlA"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(PROJECT)\n",
    "# Only fetch rows with no embedding. Bypass this query to update all rows\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  id,\n",
    "  description,\n",
    "  image_uri\n",
    "FROM\n",
    "  `{BQ_TABLE}`\n",
    "WHERE\n",
    "  ARRAY_LENGTH(text_embedding) = 0\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "query_job.result()\n",
    "destination = query_job.destination\n",
    "rows = client.list_rows(destination, max_results=1)\n",
    "print(rows.total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2oS2X5u9JIlA"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=10 #Set to 1 to clean up stragglers if rows not a multiple of batch_size\n",
    "text_embeddings, image_embeddings, ids = [], [], []\n",
    "\n",
    "for i,row in enumerate(rows):\n",
    "    print(f'\\n{i+1}: {row[\"description\"]}\\nimage_uri:{row[\"image_uri\"]}')\n",
    "    res = embed(PROJECT,row[\"description\"][:900],row[\"image_uri\"]) #API claims to supports up to 1024 chars but in practice get errors for shorter lengths\n",
    "    print(res.text_embedding[:5])\n",
    "    print(res.image_embedding[:5])\n",
    "    text_embeddings.append(res.text_embedding)\n",
    "    image_embeddings.append(res.image_embedding)\n",
    "    ids.append(row[\"id\"])\n",
    "    if len(text_embeddings) == BATCH_SIZE:\n",
    "      print(f'\\nBATCHING {BATCH_SIZE} UPDATES TO BQ...')\n",
    "      query = f\"\"\"\n",
    "      UPDATE\n",
    "        `{BQ_TABLE}`\n",
    "      SET\n",
    "        text_embedding = (\n",
    "          CASE\n",
    "            {''.join([f'WHEN id = \"{ids[i]}\" THEN {text_embeddings[i]}{chr(10)}' for i in range(len(ids))])}\n",
    "          END),\n",
    "        image_embedding = (\n",
    "          CASE\n",
    "            {''.join([f'WHEN id = \"{ids[i]}\" THEN {image_embeddings[i]}{chr(10)}' for i in range(len(ids))])}\n",
    "          END)\n",
    "      WHERE\n",
    "        id IN {str(ids).replace('[','(').replace(']',')')}\n",
    "      \"\"\"\n",
    "      start = time.time()\n",
    "      query_job = client.query(query)\n",
    "      query_job.result()  # Wait for the query to complete.\n",
    "      end = time.time()\n",
    "      print('BQ Update Time: ', end - start)\n",
    "      text_embeddings, image_embeddings, ids = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTV7Y8IzJIlA"
   },
   "source": [
    "# Future Improvements\n",
    "\n",
    "There is no batch API for multimodal embeddings as of 8-28-23. There is one for text embeddings so I expect one to be realeased in the future at which point we should migrate to that for scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "J4LFxXG3JIlA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
